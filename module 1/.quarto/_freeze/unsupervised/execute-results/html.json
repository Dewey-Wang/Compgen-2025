{
  "hash": "ae56705b8a8e05d67d5fa971fac458dc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Unsupervised Learning\"\nauthor: \"Ding Yang Wang\"\ndate: \"2025-03-05\"\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n# 2. Unsupervised Learning: Why Explore the Data?\n\nWith a statistical foundation, we know our data's basics, but we're curious: Are there hidden patterns? For example, do some genes cluster together, hinting at shared roles? Unsupervised learning explores it further by detecting inherent patterns without predefined labels. This step bridges initial data understanding to more focused investigations.\n\n## 2.1 Distance Metrics: Defining Similarity\n\nTo group data, we need a similarity rule. Clustering hinges on measuring similarity or dissimilarity between data points, which relies on **distance metrics**. The choice of metric shapes the clustering outcome, making it a critical decision. Common distance metrics include: **Euclidean Distance, Manhattan Distance, Correlation Distance, Cosine Distance, and Jaccard Distance. You can choose a proper metrics base on your data type, data characteristics, and goal.**\n\n## 2.2 Clustering: Spotting Natural Groups\n\n### 2.2.1 Hierarchical Clustering\n\nThis builds a tree structure (dendrogram), like patient subtypes, great for exploring relationships.\n\n### 2.2.2 K-means Clustering\n\nK-means splits data into k clusters based on centroids, minimizing distances, and works well for big datasets.\n\n### 2.2.3 Hierarchical Clustering vs. K-means\n\nHierarchical clustering shines in exploration but is slow (O(nÂ³)) for large data. K-means is fast and scalable, though it assumes even clusters and needs a set k.\n\n### 2.2.4 Choosing k: Picking the Right Number of Groups\n\nK-means requires k. Silhouette analysis checks fit, gap statistic compares to random data, and NbClust tools find the best number.\n\n## 2.3 Dimensionality Reduction: Seeing the Big Picture\n\nHigh-dimensional data (e.g., thousands of genes) is tough to see. Dimensionality reduction maps it to lower dimensions, revealing patterns and aiding clustering.\n\n### 2.3.1 Comparison of Techniques\n\n-   *PCA*: Linear, fast, global variance, misses non-linear patterns.\\\n-   *NMF/ICA*: Linear, specific patterns (e.g., non-negative), less visual-focused.\\\n-   *MDS*: Linear, preserves distances, balances structure.\\\n-   *t-SNE*: Non-linear, local clusters, slow and stochastic.\\\n-   *UMAP*: Non-linear, local-global balance, efficient.\n\nFor genomics, PCA fits bulk data, t-SNE/UMAP suits single-cell, and NMF helps with expression patterns.\n\n## 2.4 Overall Purpose\n\nUnsupervised learning sketches the data's natural layout, highlighting groups and structures for further study.\n\n## References\n\n-   <https://compgenomr.github.io/book/unsupervisedLearning.html>\n\n::: {style=\"display: flex; justify-content: space-between; align-items: center;\"}\n::: {style=\"text-align: left;\"}\n[Previous](statistics.qmd)\n:::\n\n::: {style=\"text-align: right;\"}\n[Next](supervised.qmd)\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}