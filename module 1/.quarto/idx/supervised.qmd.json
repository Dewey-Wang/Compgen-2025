{"title":"Supervised Learning","markdown":{"yaml":{"title":"Supervised Learning","author":"Ding Yang Wang","date":"`r Sys.Date()`"},"headingText":"3. Supervised Learning: Developing Predictive Models","containsRefs":false,"markdown":"\n\n\nWith patterns identified, supervised learning uses labeled data to build models that predict outcomes, such as disease states or treatment responses, turning insights into practical applications.\n\n## 3.1 Data Preprocessing: Ensuring Quality\n\nRaw genomic data requires preprocessing. Techniques like normalize, log transformation, remove near-zero variance (NZV) genes, imputation for missing values, and noise filtering enhance data quality, ensuring models focus on relevant signals.\n\n## 3.2 Data Splitting: Testing Real Performance\n\n### 3.2.1 Holdout Test Dataset\n\nWe split data (e.g., 70% training, 30% testing) to test on unseen samples.\n\n### 3.2.2 Cross-Validation (CV)\n\nFor small datasets, k-fold CV (e.g., 5-fold) trains and validates repeatedly, averaging results.\n\n### 3.2.3 Bootstrap Resampling\n\nBootstrap samples with replacement, using out-of-bag data for error checks, repeated for reliability.\n\n## 3.3 Predictive Models\n\nAlgorithms vary by task:\n\n```{r, echo=FALSE}\nlibrary(DT)\n\nsupervised_table <- data.frame(\n  Category = c(\n    \"Linear Regression Models\", \"\", \"\", \"\",\n    \"Non-Linear Regression Models\", \"\",\n    \"Tree-Based Methods\", \"\", \"\",\n    \"Distance-Based Methods\", \"\",\n    \"Non-Linear Advanced Methods\"\n  ),\n  Algorithm = c(\n    \"Logistic Regression\", \"Ridge Regression\", \"Lasso Regression\", \"Elastic Net\",\n    \"Polynomial Regression\", \"Generalized Additive Models (GAMs)\",\n    \"Decision Trees\", \"Random Forests\", \"Gradient Boosting (XGBoost)\",\n    \"k-Nearest Neighbors (k-NN)\", \"Support Vector Machines (SVM)\",\n    \"Neural Networks\"\n  ),\n  Assumptions = c(\n    \"Linear relationship between features and log-odds, assumes independence\",\n    \"Linear relationship, assumes multicollinearity needs regularization\",\n    \"Linear relationship, assumes sparse feature importance\",\n    \"Linear relationship, assumes a mix of sparse and correlated features\",\n    \"Non-linear relationship, assumes polynomial form captures data\",\n    \"Non-linear relationship, assumes additive non-linear effects\",\n    \"No strict distributional assumptions, handles categorical and numerical data\",\n    \"No strict distributional assumptions, benefits from randomness in data and features\",\n    \"No strict distributional assumptions, assumes sequential error correction\",\n    \"No strict assumptions, assumes similarity based on distance\",\n    \"Assumes data can be separated by a hyperplane (linear or non-linear with kernel)\",\n    \"No strict assumptions, assumes complex patterns can be learned with sufficient data\"\n  ),\n  Use_Case = c(\n    \"Disease risk prediction based on binary outcomes\",\n    \"Predicting outcomes with high-dimensional, correlated genomic data\",\n    \"Feature selection in high-dimensional datasets like genomics\",\n    \"Balanced feature selection and prediction in genomic studies\",\n    \"Modeling non-linear trends like gene expression over time\",\n    \"Capturing complex gene interactions in expression prediction\",\n    \"Medical diagnosis, simple classification or regression tasks\",\n    \"Gene expression classification, credit risk assessment\",\n    \"Customer churn prediction, competition-winning models\",\n    \"Small dataset classification, such as patient categorization\",\n    \"Image recognition, small dataset classification\",\n    \"Image and speech processing, complex genomic pattern recognition\"\n  ),\n  Advantages = c(\n    \"Simple and efficient, strong interpretability\",\n    \"Stabilizes predictions with correlated features\",\n    \"Performs feature selection by shrinking coefficients to zero\",\n    \"Combines Ridge and Lasso strengths for stability and selection\",\n    \"Simple extension of linear models, captures non-linear trends\",\n    \"Flexible modeling of multiple non-linear effects\",\n    \"Easy to understand and interpret, no data scaling needed\",\n    \"High accuracy, robust to noise, handles high-dimensional data\",\n    \"High accuracy, handles missing values, computationally efficient\",\n    \"Simple to implement, no model training required\",\n    \"Effective for small datasets, robust to noise with proper tuning\",\n    \"Learns complex patterns, flexible for large datasets\"\n  ),\n  Disadvantages = c(\n    \"Limited to linear relationships, sensitive to outliers\",\n    \"Does not perform feature selection\",\n    \"May randomly select among correlated features\",\n    \"Requires tuning of multiple parameters\",\n    \"Risk of overfitting with high-degree polynomials\",\n    \"Computationally intensive, model selection complex\",\n    \"Prone to overfitting, weak noise resistance\",\n    \"High computational cost, less interpretable\",\n    \"Complex parameter tuning, risk of overfitting\",\n    \"Slow with large datasets, sensitive to noise\",\n    \"Difficult parameter tuning, inefficient for large datasets\",\n    \"Requires large data, computationally intensive, hard to interpret\"\n  )\n)\n\ndatatable(supervised_table,\n          options = list(\n            ordering = FALSE,    # Disable sorting\n            searching = FALSE,   # Disable searching\n            paging = FALSE,      # Disable pagination\n            info = FALSE,        # Hide information\n            scrollY = FALSE,     # Disable vertical scrolling\n            scrollX = FALSE      # Disable horizontal scrolling\n          ),\n          class = 'quarto-table cell-border stripe',\n          caption = 'Supervised Learning Algorithms Including Non-Linear Regression',\n          rownames = FALSE) %>%\n  formatStyle(\n    'Category',\n    fontWeight = 'bold',\n    textAlign = 'center'\n  ) %>%\n  formatStyle(\n    'Algorithm',\n    fontWeight = 'bold',\n    color = '#003366',\n    textAlign = 'center'\n  ) %>%\n  formatStyle(\n    'Assumptions',\n    width = '25em',\n    textAlign = 'center'\n  ) %>%\n  formatStyle(\n    'Use_Case',\n    width = '25em',\n    textAlign = 'center'\n  ) %>%\n  formatStyle(\n    'Advantages',\n    width = '20em',\n    textAlign = 'center'\n  ) %>%\n  formatStyle(\n    'Disadvantages',\n    width = '20em',\n    textAlign = 'center'\n  ) %>%\n  formatStyle(\n    1:12,  # Set background colors for specific rows\n    backgroundColor = styleRow(1:12, c('#cce6ff', '#cce6ff', '#cce6ff', '#cce6ff', \n                                       '#fff0f5', '#fff0f5', \n                                       '#d9d9d9', '#d9d9d9', '#d9d9d9', \n                                       '#f9e6d2', '#f9e6d2', \n                                       '#e6ccff'))\n  )\n```\n\n## 3.4 Model Evaluation: Assessing Performance\n\nConfusion matrices and ROC curves measure model accuracy and reliability.\n\n## 3.5 Handling Challenges: Addressing Real-World Issues\n\n### 3.5.1 Class Imbalance\n\n\n-   **Sampling**: Down-sample majority or up-sample minority (e.g., SMOTE).\n-   **Case Weights**: Assign higher weights to minority class.\n-   **Threshold Adjustment**: Optimize ROC thresholds. Genomic data often has class imbalance (e.g., more healthy samples) or correlated predictors (e.g., gene pathways). Sampling, weighting, or PCA fixes these.\n\n### 3.5.2 Correlated Predictors\n\nCorrelated predictors cause multicollinearity, increasing variance:\n\n-   **PCA**: Reduces correlation, though less interpretable.\n-   **Filtering**: Removes highly correlated variables.\n-   **Regularization**: Mitigates collinearity effects.\n\n## 3.6 Overall Purpose\n\nUnlike unsupervised learning's exploratory focus, supervised learning uses labeled data for prediction, transforming patterns into practical applications. Each step, from data splitting to regularization, enhances generalization, applicable to both classification and regression, offering robust tools for genomic research.\n## References\n\n-   <https://compgenomr.github.io/book/supervisedLearning.html>\n\n::: {style=\"text-align: left;\"}\n[Previous](unsupervised.qmd)\n:::\n","srcMarkdownNoYaml":"\n\n# 3. Supervised Learning: Developing Predictive Models\n\nWith patterns identified, supervised learning uses labeled data to build models that predict outcomes, such as disease states or treatment responses, turning insights into practical applications.\n\n## 3.1 Data Preprocessing: Ensuring Quality\n\nRaw genomic data requires preprocessing. Techniques like normalize, log transformation, remove near-zero variance (NZV) genes, imputation for missing values, and noise filtering enhance data quality, ensuring models focus on relevant signals.\n\n## 3.2 Data Splitting: Testing Real Performance\n\n### 3.2.1 Holdout Test Dataset\n\nWe split data (e.g., 70% training, 30% testing) to test on unseen samples.\n\n### 3.2.2 Cross-Validation (CV)\n\nFor small datasets, k-fold CV (e.g., 5-fold) trains and validates repeatedly, averaging results.\n\n### 3.2.3 Bootstrap Resampling\n\nBootstrap samples with replacement, using out-of-bag data for error checks, repeated for reliability.\n\n## 3.3 Predictive Models\n\nAlgorithms vary by task:\n\n```{r, echo=FALSE}\nlibrary(DT)\n\nsupervised_table <- data.frame(\n  Category = c(\n    \"Linear Regression Models\", \"\", \"\", \"\",\n    \"Non-Linear Regression Models\", \"\",\n    \"Tree-Based Methods\", \"\", \"\",\n    \"Distance-Based Methods\", \"\",\n    \"Non-Linear Advanced Methods\"\n  ),\n  Algorithm = c(\n    \"Logistic Regression\", \"Ridge Regression\", \"Lasso Regression\", \"Elastic Net\",\n    \"Polynomial Regression\", \"Generalized Additive Models (GAMs)\",\n    \"Decision Trees\", \"Random Forests\", \"Gradient Boosting (XGBoost)\",\n    \"k-Nearest Neighbors (k-NN)\", \"Support Vector Machines (SVM)\",\n    \"Neural Networks\"\n  ),\n  Assumptions = c(\n    \"Linear relationship between features and log-odds, assumes independence\",\n    \"Linear relationship, assumes multicollinearity needs regularization\",\n    \"Linear relationship, assumes sparse feature importance\",\n    \"Linear relationship, assumes a mix of sparse and correlated features\",\n    \"Non-linear relationship, assumes polynomial form captures data\",\n    \"Non-linear relationship, assumes additive non-linear effects\",\n    \"No strict distributional assumptions, handles categorical and numerical data\",\n    \"No strict distributional assumptions, benefits from randomness in data and features\",\n    \"No strict distributional assumptions, assumes sequential error correction\",\n    \"No strict assumptions, assumes similarity based on distance\",\n    \"Assumes data can be separated by a hyperplane (linear or non-linear with kernel)\",\n    \"No strict assumptions, assumes complex patterns can be learned with sufficient data\"\n  ),\n  Use_Case = c(\n    \"Disease risk prediction based on binary outcomes\",\n    \"Predicting outcomes with high-dimensional, correlated genomic data\",\n    \"Feature selection in high-dimensional datasets like genomics\",\n    \"Balanced feature selection and prediction in genomic studies\",\n    \"Modeling non-linear trends like gene expression over time\",\n    \"Capturing complex gene interactions in expression prediction\",\n    \"Medical diagnosis, simple classification or regression tasks\",\n    \"Gene expression classification, credit risk assessment\",\n    \"Customer churn prediction, competition-winning models\",\n    \"Small dataset classification, such as patient categorization\",\n    \"Image recognition, small dataset classification\",\n    \"Image and speech processing, complex genomic pattern recognition\"\n  ),\n  Advantages = c(\n    \"Simple and efficient, strong interpretability\",\n    \"Stabilizes predictions with correlated features\",\n    \"Performs feature selection by shrinking coefficients to zero\",\n    \"Combines Ridge and Lasso strengths for stability and selection\",\n    \"Simple extension of linear models, captures non-linear trends\",\n    \"Flexible modeling of multiple non-linear effects\",\n    \"Easy to understand and interpret, no data scaling needed\",\n    \"High accuracy, robust to noise, handles high-dimensional data\",\n    \"High accuracy, handles missing values, computationally efficient\",\n    \"Simple to implement, no model training required\",\n    \"Effective for small datasets, robust to noise with proper tuning\",\n    \"Learns complex patterns, flexible for large datasets\"\n  ),\n  Disadvantages = c(\n    \"Limited to linear relationships, sensitive to outliers\",\n    \"Does not perform feature selection\",\n    \"May randomly select among correlated features\",\n    \"Requires tuning of multiple parameters\",\n    \"Risk of overfitting with high-degree polynomials\",\n    \"Computationally intensive, model selection complex\",\n    \"Prone to overfitting, weak noise resistance\",\n    \"High computational cost, less interpretable\",\n    \"Complex parameter tuning, risk of overfitting\",\n    \"Slow with large datasets, sensitive to noise\",\n    \"Difficult parameter tuning, inefficient for large datasets\",\n    \"Requires large data, computationally intensive, hard to interpret\"\n  )\n)\n\ndatatable(supervised_table,\n          options = list(\n            ordering = FALSE,    # Disable sorting\n            searching = FALSE,   # Disable searching\n            paging = FALSE,      # Disable pagination\n            info = FALSE,        # Hide information\n            scrollY = FALSE,     # Disable vertical scrolling\n            scrollX = FALSE      # Disable horizontal scrolling\n          ),\n          class = 'quarto-table cell-border stripe',\n          caption = 'Supervised Learning Algorithms Including Non-Linear Regression',\n          rownames = FALSE) %>%\n  formatStyle(\n    'Category',\n    fontWeight = 'bold',\n    textAlign = 'center'\n  ) %>%\n  formatStyle(\n    'Algorithm',\n    fontWeight = 'bold',\n    color = '#003366',\n    textAlign = 'center'\n  ) %>%\n  formatStyle(\n    'Assumptions',\n    width = '25em',\n    textAlign = 'center'\n  ) %>%\n  formatStyle(\n    'Use_Case',\n    width = '25em',\n    textAlign = 'center'\n  ) %>%\n  formatStyle(\n    'Advantages',\n    width = '20em',\n    textAlign = 'center'\n  ) %>%\n  formatStyle(\n    'Disadvantages',\n    width = '20em',\n    textAlign = 'center'\n  ) %>%\n  formatStyle(\n    1:12,  # Set background colors for specific rows\n    backgroundColor = styleRow(1:12, c('#cce6ff', '#cce6ff', '#cce6ff', '#cce6ff', \n                                       '#fff0f5', '#fff0f5', \n                                       '#d9d9d9', '#d9d9d9', '#d9d9d9', \n                                       '#f9e6d2', '#f9e6d2', \n                                       '#e6ccff'))\n  )\n```\n\n## 3.4 Model Evaluation: Assessing Performance\n\nConfusion matrices and ROC curves measure model accuracy and reliability.\n\n## 3.5 Handling Challenges: Addressing Real-World Issues\n\n### 3.5.1 Class Imbalance\n\n\n-   **Sampling**: Down-sample majority or up-sample minority (e.g., SMOTE).\n-   **Case Weights**: Assign higher weights to minority class.\n-   **Threshold Adjustment**: Optimize ROC thresholds. Genomic data often has class imbalance (e.g., more healthy samples) or correlated predictors (e.g., gene pathways). Sampling, weighting, or PCA fixes these.\n\n### 3.5.2 Correlated Predictors\n\nCorrelated predictors cause multicollinearity, increasing variance:\n\n-   **PCA**: Reduces correlation, though less interpretable.\n-   **Filtering**: Removes highly correlated variables.\n-   **Regularization**: Mitigates collinearity effects.\n\n## 3.6 Overall Purpose\n\nUnlike unsupervised learning's exploratory focus, supervised learning uses labeled data for prediction, transforming patterns into practical applications. Each step, from data splitting to regularization, enhances generalization, applicable to both classification and regression, offering robust tools for genomic research.\n## References\n\n-   <https://compgenomr.github.io/book/supervisedLearning.html>\n\n::: {style=\"text-align: left;\"}\n[Previous](unsupervised.qmd)\n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":true,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":2,"output-file":"supervised.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","theme":"cosmo","title":"Supervised Learning","author":"Ding Yang Wang","date":"`r Sys.Date()`"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}