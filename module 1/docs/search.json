[
  {
    "objectID": "statistics.html",
    "href": "statistics.html",
    "title": "Statistics for Genomics",
    "section": "",
    "text": "Genomic research begins with complex datasets, such as gene expression levels, which contain natural variation and experimental noise. Statistics provides a systematic way to process and interpret this data, ensuring a dependable foundation for further analysis. Below are the key statistical steps and their roles:\n\n\nGene expression data often varies significantly. We need a ‚Äútypical‚Äù value to grasp the big picture. Measures like the mean and median summarize the central tendency. The mean uses all data but can be swayed by outliers; the median stays steady, perfect for uneven datasets. Variance and standard deviation quantify the data‚Äôs spread, indicating consistency or highlighting potential biological diversity or experimental errors.\n\n\n\nData frequently follows patterns, such as the normal distribution for continuous traits like gene expression or the Poisson distribution for discrete counts like mutations. Identifying these distributions guides the selection of appropriate analytical methods, many of which rely on specific assumptions about data shape.\n\n\n\nWe estimate population traits (e.g., average gene expression) from samples. Confidence intervals give a range (e.g., 95% likely to hold the true value), showing how much trust we can place in our estimate.\n\n\n\nWe compare groups, like healthy vs.¬†diseased cells. Hypothesis testing (e.g., t-tests, Moderated t-test, ANOVA, Wilcoxon Signed-Rank Test, Mann-Whitney U Test) uses p-values to check if differences are random. Small p-values (&lt; 0.05) suggest real effects. With thousands of genes, multiple testing corrections (e.g., Bonferroni, FDR) curb false positives.\n\n\n\n\n\n\n\n\n\nStatistics turns raw data into a trustworthy base, revealing what‚Äôs happening and what‚Äôs worth investigating.\n\n\n\n\nhttps://compgenomr.github.io/book/stats.html\n\n\nNext"
  },
  {
    "objectID": "statistics.html#summarizing-data-finding-typical-and-spread",
    "href": "statistics.html#summarizing-data-finding-typical-and-spread",
    "title": "Statistics for Genomics",
    "section": "",
    "text": "Gene expression data often varies significantly. We need a ‚Äútypical‚Äù value to grasp the big picture. Measures like the mean and median summarize the central tendency. The mean uses all data but can be swayed by outliers; the median stays steady, perfect for uneven datasets. Variance and standard deviation quantify the data‚Äôs spread, indicating consistency or highlighting potential biological diversity or experimental errors."
  },
  {
    "objectID": "statistics.html#distribution-models-understanding-data-shape",
    "href": "statistics.html#distribution-models-understanding-data-shape",
    "title": "Statistics for Genomics",
    "section": "",
    "text": "Data frequently follows patterns, such as the normal distribution for continuous traits like gene expression or the Poisson distribution for discrete counts like mutations. Identifying these distributions guides the selection of appropriate analytical methods, many of which rely on specific assumptions about data shape."
  },
  {
    "objectID": "statistics.html#confidence-intervals-measuring-uncertainty",
    "href": "statistics.html#confidence-intervals-measuring-uncertainty",
    "title": "Statistics for Genomics",
    "section": "",
    "text": "We estimate population traits (e.g., average gene expression) from samples. Confidence intervals give a range (e.g., 95% likely to hold the true value), showing how much trust we can place in our estimate."
  },
  {
    "objectID": "statistics.html#hypothesis-testing-confirming-real-differences",
    "href": "statistics.html#hypothesis-testing-confirming-real-differences",
    "title": "Statistics for Genomics",
    "section": "",
    "text": "We compare groups, like healthy vs.¬†diseased cells. Hypothesis testing (e.g., t-tests, Moderated t-test, ANOVA, Wilcoxon Signed-Rank Test, Mann-Whitney U Test) uses p-values to check if differences are random. Small p-values (&lt; 0.05) suggest real effects. With thousands of genes, multiple testing corrections (e.g., Bonferroni, FDR) curb false positives."
  },
  {
    "objectID": "statistics.html#overall-purpose",
    "href": "statistics.html#overall-purpose",
    "title": "Statistics for Genomics",
    "section": "",
    "text": "Statistics turns raw data into a trustworthy base, revealing what‚Äôs happening and what‚Äôs worth investigating."
  },
  {
    "objectID": "statistics.html#references",
    "href": "statistics.html#references",
    "title": "Statistics for Genomics",
    "section": "",
    "text": "https://compgenomr.github.io/book/stats.html\n\n\nNext"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Genomic Insights",
    "section": "",
    "text": "This website is my personal note compilation, based on the Compgen2025 ‚Äì Module 1: AI-assisted Data Analysis course. The course content primarily covers statistical foundations and machine learning applications in genomics. If you are interested in the application of ML in bioinformatics, you may find this resource helpful!\nThe course consists of a series of theoretical lectures and programming exercises, with a primary focus on analysis using R. However, in my notes, I will mainly cover fundamental theoretical concepts and explain the reasoning behind each step rather than focusing on code implementation."
  },
  {
    "objectID": "index.html#course-modules",
    "href": "index.html#course-modules",
    "title": "Welcome to Genomic Insights",
    "section": "Course Modules:",
    "text": "Course Modules:\n\nModule 1: AI-assisted data analysis (This note only cover this module right now)\nModule 2: Spatial omics data analysis\nModule 3: Multi-omics data integration\n\nüîó More details about the course here"
  },
  {
    "objectID": "unsupervised.html",
    "href": "unsupervised.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "With a statistical foundation, we know our data‚Äôs basics, but we‚Äôre curious: Are there hidden patterns? For example, do some genes cluster together, hinting at shared roles? Unsupervised learning explores it further by detecting inherent patterns without predefined labels. This step bridges initial data understanding to more focused investigations.\n\n\nTo group data, we need a similarity rule. Clustering hinges on measuring similarity or dissimilarity between data points, which relies on distance metrics. The choice of metric shapes the clustering outcome, making it a critical decision. Common distance metrics include: Euclidean Distance, Manhattan Distance, Correlation Distance, Cosine Distance, and Jaccard Distance. You can choose a proper metrics base on your data type, data characteristics, and goal.\n\n\n\n\n\nThis builds a tree structure (dendrogram), like patient subtypes, great for exploring relationships.\n\n\n\nK-means splits data into k clusters based on centroids, minimizing distances, and works well for big datasets.\n\n\n\nHierarchical clustering shines in exploration but is slow (O(n¬≥)) for large data. K-means is fast and scalable, though it assumes even clusters and needs a set k.\n\n\n\nK-means requires k. Silhouette analysis checks fit, gap statistic compares to random data, and NbClust tools find the best number.\n\n\n\n\nHigh-dimensional data (e.g., thousands of genes) is tough to see. Dimensionality reduction maps it to lower dimensions, revealing patterns and aiding clustering.\n\n\n\nPCA: Linear, fast, global variance, misses non-linear patterns.\n\nNMF/ICA: Linear, specific patterns (e.g., non-negative), less visual-focused.\n\nMDS: Linear, preserves distances, balances structure.\n\nt-SNE: Non-linear, local clusters, slow and stochastic.\n\nUMAP: Non-linear, local-global balance, efficient.\n\nFor genomics, PCA fits bulk data, t-SNE/UMAP suits single-cell, and NMF helps with expression patterns.\n\n\n\n\nUnsupervised learning sketches the data‚Äôs natural layout, highlighting groups and structures for further study.\n\n\n\n\nhttps://compgenomr.github.io/book/unsupervisedLearning.html\n\n\n\nPrevious\n\n\nNext"
  },
  {
    "objectID": "unsupervised.html#distance-metrics-defining-similarity",
    "href": "unsupervised.html#distance-metrics-defining-similarity",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "To group data, we need a similarity rule. Clustering hinges on measuring similarity or dissimilarity between data points, which relies on distance metrics. The choice of metric shapes the clustering outcome, making it a critical decision. Common distance metrics include: Euclidean Distance, Manhattan Distance, Correlation Distance, Cosine Distance, and Jaccard Distance. You can choose a proper metrics base on your data type, data characteristics, and goal."
  },
  {
    "objectID": "unsupervised.html#clustering-spotting-natural-groups",
    "href": "unsupervised.html#clustering-spotting-natural-groups",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "This builds a tree structure (dendrogram), like patient subtypes, great for exploring relationships.\n\n\n\nK-means splits data into k clusters based on centroids, minimizing distances, and works well for big datasets.\n\n\n\nHierarchical clustering shines in exploration but is slow (O(n¬≥)) for large data. K-means is fast and scalable, though it assumes even clusters and needs a set k.\n\n\n\nK-means requires k. Silhouette analysis checks fit, gap statistic compares to random data, and NbClust tools find the best number."
  },
  {
    "objectID": "unsupervised.html#dimensionality-reduction-seeing-the-big-picture",
    "href": "unsupervised.html#dimensionality-reduction-seeing-the-big-picture",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "High-dimensional data (e.g., thousands of genes) is tough to see. Dimensionality reduction maps it to lower dimensions, revealing patterns and aiding clustering.\n\n\n\nPCA: Linear, fast, global variance, misses non-linear patterns.\n\nNMF/ICA: Linear, specific patterns (e.g., non-negative), less visual-focused.\n\nMDS: Linear, preserves distances, balances structure.\n\nt-SNE: Non-linear, local clusters, slow and stochastic.\n\nUMAP: Non-linear, local-global balance, efficient.\n\nFor genomics, PCA fits bulk data, t-SNE/UMAP suits single-cell, and NMF helps with expression patterns."
  },
  {
    "objectID": "unsupervised.html#overall-purpose",
    "href": "unsupervised.html#overall-purpose",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Unsupervised learning sketches the data‚Äôs natural layout, highlighting groups and structures for further study."
  },
  {
    "objectID": "unsupervised.html#references",
    "href": "unsupervised.html#references",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "https://compgenomr.github.io/book/unsupervisedLearning.html\n\n\n\nPrevious\n\n\nNext"
  },
  {
    "objectID": "supervised.html",
    "href": "supervised.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "With patterns identified, supervised learning uses labeled data to build models that predict outcomes, such as disease states or treatment responses, turning insights into practical applications.\n\n\nRaw genomic data requires preprocessing. Techniques like normalize, log transformation, remove near-zero variance (NZV) genes, imputation for missing values, and noise filtering enhance data quality, ensuring models focus on relevant signals.\n\n\n\n\n\nWe split data (e.g., 70% training, 30% testing) to test on unseen samples.\n\n\n\nFor small datasets, k-fold CV (e.g., 5-fold) trains and validates repeatedly, averaging results.\n\n\n\nBootstrap samples with replacement, using out-of-bag data for error checks, repeated for reliability.\n\n\n\n\nAlgorithms vary by task:\n\n\n\n\n\n\n\n\n\nConfusion matrices and ROC curves measure model accuracy and reliability.\n\n\n\n\n\nImbalance (e.g., more healthy samples) skews results. Sampling (e.g., SMOTE), weighting, or threshold adjustments fix this.\n\n\n\nCorrelated genes cause multicollinearity. PCA, filtering, or regularization address it.\n\n\n\n\nSupervised learning predicts outcomes with labeled data, refining tools through preprocessing and validation for genomics.\n\n\n\n\nhttps://compgenomr.github.io/book/supervisedLearning.html\n\n\nPrevious"
  },
  {
    "objectID": "supervised.html#data-preprocessing-ensuring-quality",
    "href": "supervised.html#data-preprocessing-ensuring-quality",
    "title": "Supervised Learning",
    "section": "",
    "text": "Raw genomic data requires preprocessing. Techniques like normalize, log transformation, remove near-zero variance (NZV) genes, imputation for missing values, and noise filtering enhance data quality, ensuring models focus on relevant signals."
  },
  {
    "objectID": "supervised.html#data-splitting-testing-real-performance",
    "href": "supervised.html#data-splitting-testing-real-performance",
    "title": "Supervised Learning",
    "section": "",
    "text": "We split data (e.g., 70% training, 30% testing) to test on unseen samples.\n\n\n\nFor small datasets, k-fold CV (e.g., 5-fold) trains and validates repeatedly, averaging results.\n\n\n\nBootstrap samples with replacement, using out-of-bag data for error checks, repeated for reliability."
  },
  {
    "objectID": "supervised.html#predictive-models",
    "href": "supervised.html#predictive-models",
    "title": "Supervised Learning",
    "section": "",
    "text": "Algorithms vary by task:"
  },
  {
    "objectID": "supervised.html#model-evaluation-assessing-performance",
    "href": "supervised.html#model-evaluation-assessing-performance",
    "title": "Supervised Learning",
    "section": "",
    "text": "Confusion matrices and ROC curves measure model accuracy and reliability."
  },
  {
    "objectID": "supervised.html#handling-challenges-addressing-real-world-issues",
    "href": "supervised.html#handling-challenges-addressing-real-world-issues",
    "title": "Supervised Learning",
    "section": "",
    "text": "Imbalance (e.g., more healthy samples) skews results. Sampling (e.g., SMOTE), weighting, or threshold adjustments fix this.\n\n\n\nCorrelated genes cause multicollinearity. PCA, filtering, or regularization address it."
  },
  {
    "objectID": "supervised.html#overall-purpose",
    "href": "supervised.html#overall-purpose",
    "title": "Supervised Learning",
    "section": "",
    "text": "Supervised learning predicts outcomes with labeled data, refining tools through preprocessing and validation for genomics."
  },
  {
    "objectID": "supervised.html#references",
    "href": "supervised.html#references",
    "title": "Supervised Learning",
    "section": "",
    "text": "https://compgenomr.github.io/book/supervisedLearning.html\n\n\nPrevious"
  }
]